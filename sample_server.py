import streamlit as st
from streamlit_local_storage import LocalStorage

import os
import json
import time
import asyncio
from dotenv import load_dotenv
from typing import List, Dict, AsyncGenerator
from datetime import datetime
from pathlib import Path
import tiktoken

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langgraph.prebuilt import create_react_agent

from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ ---
load_dotenv()

# -----------------------------------------------------------------------------
# ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
# -----------------------------------------------------------------------------

# --- ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
MCP_CONFIG_FILE = "mcp.json"
HISTORY_DIR = Path("chat_histories")
HISTORY_DIR.mkdir(exist_ok=True) # ëŒ€í™” ê¸°ë¡ ì €ì¥ í´ë” ìƒì„±

global selected_category
global selected_item
selected_category = None
selected_item = None
llm_options = {
    "OpenAI":['gpt-4.1-nano','gpt-4.1-mini','gpt-4.1','gpt-4o','o4-mini','o3','o3-mini','o1','o1-mini'],
    "Gemini":['gemini-2.0-flash-001','gemini-2.5-flash','gemini-1.5-flash'],
    "Claude":['claude-3-7-sonnet-20250219', 'claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022','claude-3-5-sonnet-20240620','claude-sonnet-4-20250514']
}
#'claude-opus-4-20250514'

# --- í—¬í¼ í•¨ìˆ˜ ---
# <<< [ìˆ˜ì •] í† í° ê³„ì‚° í—¬í¼ í•¨ìˆ˜ ì¶”ê°€ ì‹œì‘ >>>
def count_tokens(text: str, model: str = "gpt-4") -> int:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë¸ì— ë§ëŠ” ì¸ì½”ë”© ë°©ì‹ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        # ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš° ê¸°ë³¸ ì¸ì½”ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def generate_filename_with_timestamp(prefix="chat_", extension="json"):
    """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    now = datetime.now()
    timestamp_str = now.strftime("%Y%m%d_%H%M%S")
    if prefix:
        filename = f"{prefix}{timestamp_str}.{extension}"
    else:
        filename = f"{timestamp_str}.{extension}"
    return filename

# @st.cache_resource
def get_llm():
    """LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    if selected_category == 'Claude':
        llm = ChatAnthropic(model=selected_item, temperature=0, max_tokens=4096)
    elif selected_category == 'OpenAI':
        llm = ChatOpenAI(model=selected_item, max_tokens=8000)
    elif selected_category == 'Gemini':
        llm = ChatGoogleGenerativeAI(model=selected_item)
    else:
        llm = ChatOpenAI(model="o4-mini", temperature=0,  max_tokens=8000)
    return llm

# @st.cache_data
def load_mcp_config():
    """mcp.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_mcp_config(config):
    """MCP ì„œë²„ ì„¤ì •ì„ mcp.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(MCP_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---
async def select_mcp_servers(query: str, servers_config: Dict) -> List[str]:
    """ì‚¬ìš©ì ì§ˆì˜ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©í•  MCP ì„œë²„ë¥¼ LLMì„ í†µí•´ ì„ íƒí•©ë‹ˆë‹¤."""
    llm = get_llm()
    active_servers = {name: config for name, config in servers_config.items() if config.get("active", True)}

    if not active_servers:
        st.info("í˜„ì¬ í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    system_prompt = "You are a helpful assistant that selects the most relevant tools for a given user query. ë‚˜ì˜ Instructionì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì ˆëŒ€ ëŒ€ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    prompt_template = """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ê·¸ 'description'ì„ ë³´ê³  ì„ íƒí•´ì£¼ì„¸ìš”.
    ì„ íƒëœ ë„êµ¬ì˜ ì´ë¦„(í‚¤ ê°’)ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ëª©ë¡ìœ¼ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. (ì˜ˆ: weather,Home Assistant)
    ë§Œì•½ ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´ 'None'ì´ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”.

    [ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡]
    {tools_description}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    [ì„ íƒëœ ë„êµ¬ ëª©ë¡]
    """
    descriptions = "\n".join([f"- {name}: {config['description']}" for name, config in active_servers.items()])
    prompt = ChatPromptTemplate.from_template(prompt_template).format(
        tools_description=descriptions,
        user_query=query
    )
    response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])
    selected = [s.strip() for s in response.content.split(',') if s.strip() and s.strip().lower() != 'none']
    return selected

# (â˜…â˜…â˜…â˜…â˜… ë¡œì§ ìˆ˜ì • â˜…â˜…â˜…â˜…â˜…)
async def process_query(query: str, chat_history: List) -> AsyncGenerator[str, None]:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ ì„œë²„ ì„ íƒ, ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰ì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    'cancel scope' ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ ë°©ì‹ì„ ainvokeë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    """

    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ì‹œì‘ >>>
    MAX_HISTORY_TOKENS = 4096  # LLMì— ì „ë‹¬í•  ìµœëŒ€ íˆìŠ¤í† ë¦¬ í† í° ìˆ˜ ì œí•œ

    history_for_llm = []
    current_tokens = 0

    # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ë©° í† í° ìˆ˜ë¥¼ í™•ì¸
    for message in reversed(chat_history):
        message_content = message.content
        # í˜„ì¬ ë©”ì‹œì§€ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°
        message_tokens = count_tokens(message_content)

        # ì´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë©´ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
        if current_tokens + message_tokens > MAX_HISTORY_TOKENS:
            # ë„˜ëŠ”ë‹¤ë©´ ë” ì´ìƒ ì´ì „ ê¸°ë¡ì„ ì¶”ê°€í•˜ì§€ ì•Šê³  ì¢…ë£Œ
            break

        # í† í° ìˆ˜ ì œí•œì„ ë„˜ì§€ ì•Šìœ¼ë©´ ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ìˆœì„œë¥¼ ìœ„í•´ ë§¨ ì•ì— ì‚½ì…)
        history_for_llm.insert(0, message)
        current_tokens += message_tokens
    # <<< [ìˆ˜ì •] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë¡œì§ ë >>>

    mcp_config = load_mcp_config()["mcpServers"]
    llm = get_llm()
    #agent_input = {"messages": chat_history + [HumanMessage(content=query)]}
    agent_input = {"messages": history_for_llm + [HumanMessage(content=query)]}

    # 1. MCP ì„œë²„ ë¼ìš°íŒ…
    st.write("`1. MCP ì„œë²„ ë¼ìš°íŒ… ì¤‘...`")
    selected_server_names = await select_mcp_servers(query, mcp_config)

    # 2. ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ì„ ê²½ìš°, LLMìœ¼ë¡œ ì§ì ‘ ì§ˆì˜
    if not selected_server_names:
        st.info("âœ… LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
        async for chunk in llm.astream(agent_input["messages"]):
            yield chunk.content
        return

    # 3. ë‹¨ì¼ ì„œë²„ ì‹¤í–‰
    if len(selected_server_names) == 1:
        name = selected_server_names[0]
        config = mcp_config[name]
        st.write(f"`3. ë‹¨ì¼ ì„œë²„ '{name}'ì— ì—°ê²°í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.`")
        
        try:
            conn_type = config.get("transport")
            
            #final_output = f"ì—ì´ì „íŠ¸ '{name}'ê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            async def process_connection_and_stream(read, write):
                """ì„¸ì…˜ ë‚´ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    tools = await load_mcp_tools(session)
                    if not tools:
                        st.warning(f"âœ… '{name}' ì„œë²„ì— ì—°ê²°í–ˆìœ¼ë‚˜, ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        yield f"'{name}' ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                        return

                    st.success(f"âœ… '{name}' ì„œë²„ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ì„±ê³µ: `{[tool.name for tool in tools]}`")
                    agent = create_react_agent(llm, tools)
                    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)

                    # ì±„íŒ… ê¸°ë¡ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì„¤ì • (ì„ íƒ ì‚¬í•­)
                    message_history = ChatMessageHistory()

                    agent_with_chat_history = RunnableWithMessageHistory(
                        agent_executor,
                        lambda session_id: message_history,
                        input_messages_key="input",
                        history_messages_key="chat_history",
                    )

                    st.write("`4. ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...`")
                    with st.spinner(f"'{name}' ì—ì´ì „íŠ¸ê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ì¤‘ì…ë‹ˆë‹¤..."):
                        final_answer = ""
                        # astream_eventsë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ë°›ìŠµë‹ˆë‹¤.
                        async for event in agent_with_chat_history.astream_events(
                            agent_input,
                            config={"configurable": {"session_id": "test_session"}},
                            version="v2",
                        ):
                            kind = event["event"]
                            
                            # ì—ì´ì „íŠ¸ì˜ ì¤‘ê°„ ìƒê°(thought) ì¶œë ¥
                            # if kind == "on_chain_start" and event["name"] == "Agent":
                            #     print("\nğŸ”„ Agent Start")
                                
                            # LLMì´ ìƒì„±í•˜ëŠ” ì‘ë‹µ ì²­í¬(chunk)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥
                            if kind == "on_chat_model_stream":
                                content = event["data"]["chunk"].content
                                if content:
                                    # print(content, end="", flush=True)
                                    yield content
                                    final_answer += content

                            # ë„êµ¬ ì‚¬ìš© ì¢…ë£Œ ì‹œ ì¶œë ¥
                            # elif kind == "on_tool_end":
                            #     print(f"\nâœ… Tool Output: {event['data'].get('output')}")
                            #     print("\n---\nAgent is thinking...", end="", flush=True)

                        # print("\n\n--- ìµœì¢… ë‹µë³€ ---")
                        yield final_answer

            # Transport íƒ€ì…ì— ë”°ë¼ ì—°ê²° ë° ì‹¤í–‰
            if conn_type == "stdio":
                params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                async with stdio_client(params) as (read, write):
                    async for content_part in process_connection_and_stream(read, write):
                        yield content_part
            elif conn_type == "sse":
                url = config.get("url")
                headers = config.get("headers", {})
                async with sse_client(url, headers=headers) as (read, write):
                    async for content_part in process_connection_and_stream(read, write):
                        yield content_part
            else:
                st.warning(f"âš ï¸ '{name}' ì„œë²„ì˜ ì—°ê²° íƒ€ì… ('{conn_type}')ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                yield f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—°ê²° íƒ€ì…: '{conn_type}'"
               
        except Exception as e:
            if "Attempted to exit cancel scope in a different task than it was entered in" in str(e):                
                pass
            else:                
                st.error(f"âŒ '{name}' ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                yield f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
        return # ë‹¨ì¼ ì„œë²„ ì‹¤í–‰ í›„ í•¨ìˆ˜ ì¢…ë£Œ

    # 4. ë©€í‹° ì„œë²„ ì‹¤í–‰
    if len(selected_server_names) > 1:
        st.write(f"`3. ë‹¤ì¤‘ ì„œë²„ ({', '.join(selected_server_names)})ì— ì—°ê²°í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰í•©ë‹ˆë‹¤.`")

        async def run_one_agent_and_get_output(name: str) -> tuple[str, str]:
            """í•˜ë‚˜ì˜ ì„œë²„ì— ì—°ê²°í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜í•˜ëŠ” ì½”ë£¨í‹´"""
            config = mcp_config[name]
            final_output = f"[{name}] Agent failed to produce a result."
            try:
                conn_type = config.get("transport")
                
                async def get_output_from_session(read, write):
                    nonlocal final_output
                    async with ClientSession(read, write) as session:
                        await session.initialize()
                        tools = await load_mcp_tools(session)
                        if not tools:
                            st.warning(f"No tools for {name}")
                            return
                        
                        st.success(f"âœ… '{name}' ì„œë²„ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ì„±ê³µ.")
                        agent = create_react_agent(llm, tools)
                        result = await agent.ainvoke(agent_input)
                        if 'output' in result:
                            final_output = result['output']
                        elif 'messages' in result and isinstance(result['messages'][-1], AIMessage):
                            final_output = result['messages'][-1].content
                
                if conn_type == "stdio":
                    params = StdioServerParameters(command=config.get("command"), args=config.get("args", []))
                    async with stdio_client(params) as (read, write):
                        await get_output_from_session(read, write)
                elif conn_type == "sse":
                    url = config.get("url")
                    headers = config.get("headers", {})
                    async with sse_client(url, headers=headers) as (read, write):
                        await get_output_from_session(read, write)
            except Exception as e:
                st.error(f"âŒ '{name}' ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                final_output = f"[{name}] Agent execution failed with an error."
            return name, final_output

        tasks = [run_one_agent_and_get_output(name) for name in selected_server_names]
        results = await asyncio.gather(*tasks)

        # <<< [ìˆ˜ì •] ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‘ë‹µ ê°’ ì²˜ë¦¬ ë¡œì§ ì‹œì‘ >>>
        MAX_RESPONSE_TOKENS_PER_AGENT = 1500  # ê° ì—ì´ì „íŠ¸ë³„ ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜
        final_responses = {}

        for name, output in results:
            if not output:
                final_responses[name] = "ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                continue
            
            # ê° ì‘ë‹µì˜ í† í° ìˆ˜ í™•ì¸
            if count_tokens(output) > MAX_RESPONSE_TOKENS_PER_AGENT:
                # í† í° ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ë‹µ ìë¥´ê¸°
                try:
                    encoding = tiktoken.encoding_for_model(get_llm().model_name)
                except KeyError:
                    encoding = tiktoken.get_encoding("cl100k_base")
                
                tokens = encoding.encode(output)
                truncated_tokens = tokens[:MAX_RESPONSE_TOKENS_PER_AGENT]
                truncated_output = encoding.decode(truncated_tokens)
                final_responses[name] = truncated_output + "\n\n... [ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤]"
            else:
                final_responses[name] = output
        # <<< [ìˆ˜ì •] ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‘ë‹µ ê°’ ì²˜ë¦¬ ë¡œì§ ë >>>

        st.write("`4. ëª¨ë“  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ. ìµœì¢… ë‹µë³€ ì¢…í•© ì¤‘...`")
        st.json(final_responses)
        
        history_str = "\n".join([f"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}" for m in chat_history])
        synthesis_prompt_template = """
        ë‹¹ì‹ ì€ ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë§ˆìŠ¤í„° AIì…ë‹ˆë‹¤.
        ì•„ë˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë‚˜ì˜ ì¼ê´€ë˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.
        [ëŒ€í™” ê¸°ë¡]
        {chat_history}
        [ì‚¬ìš©ì í˜„ì¬ ì§ˆë¬¸]
        {original_query}
        [ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ]
        {agent_responses}
        [ì¢…í•©ëœ ìµœì¢… ë‹µë³€]
        """
        synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template)
        synthesis_chain = synthesis_prompt | llm | StrOutputParser()
        async for chunk in synthesis_chain.astream({
            "chat_history": history_str,
            "original_query": query,
            "agent_responses": json.dumps(final_responses, ensure_ascii=False, indent=2)
        }):
            yield chunk

# --- Streamlit UI êµ¬ì„± (ì´í•˜ ë³€ê²½ ì—†ìŒ) ---

st.set_page_config(page_title="MCP Client on Streamlit", layout="wide")
st.title("ğŸ¤– MCP Client")

# 1. ì¸ì¦ ì²˜ë¦¬
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if password == os.getenv("APP_PASSWORD"):
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# 2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ì¸ì¦ í›„)
with st.sidebar:
    st.header("ë©”ë‰´")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    def start_new_chat():
        st.session_state.messages = []
        st.session_state.current_chat_file = None

    def auto_save_chat():
        if st.session_state.get("current_chat_file") and st.session_state.get("messages"):
            save_path = HISTORY_DIR / st.session_state.current_chat_file
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

    def load_chat(filename: str):
        load_path = HISTORY_DIR / filename
        with open(load_path, "r", encoding="utf-8") as f:
            st.session_state.messages = json.load(f)
        st.session_state.current_chat_file = filename

    def delete_chat(filename: str):
        if st.session_state.get("current_chat_file") == filename:
            start_new_chat()
        file_to_delete = HISTORY_DIR / filename
        if file_to_delete.exists():
            file_to_delete.unlink()
            st.toast(f"'{filename}'ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    st.button("ìƒˆë¡œìš´ ì±„íŒ… ì—´ê¸°", on_click=start_new_chat, use_container_width=True)
    st.divider()
    localS = LocalStorage()

    #localStorageì—ì„œ ì´ì „ì— ì €ì¥ëœ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
    saved_model = localS.getItem("selected_model")
    if saved_model:
        saved_category =  saved_model[0]
        saved_item = saved_model[1]
    else:
        saved_category = ""
        saved_item = ""

    #1. ì²« ë²ˆì§¸ selectbox(ì¹´í…Œê³ ë¦¬)ì˜ ê¸°ë³¸ ì¸ë±ìŠ¤ ì„¤ì •
    categories = list(llm_options.keys())
    #2. ì €ì¥ëœ ê°’ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê°’ì˜ ì¸ë±ìŠ¤ë¥¼, ì—†ìœ¼ë©´ 0ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    category_index = categories.index(saved_category) if saved_category in categories else 0

    st.header("LLM ê´€ë¦¬")  

    # ì¹´í…Œê³ ë¦¬ selectbox ìƒì„±
    selected_category = st.selectbox(
        "LLMë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        categories,
        index=category_index
    )

    # 3. ë‘ ë²ˆì§¸ selectbox(ëª¨ë¸)ì˜ ê¸°ë³¸ ì¸ë±ìŠ¤ ì„¤ì •
    model_options = llm_options[selected_category]
    # ì €ì¥ëœ ëª¨ë¸ì´ í˜„ì¬ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸ í›„ ì¸ë±ìŠ¤ ì„¤ì •
    item_index = model_options.index(saved_item) if saved_item in model_options else 0

    # ëª¨ë¸ selectbox ìƒì„±
    selected_item = st.selectbox(
        f"{selected_category} ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”:",
        model_options,
        index=item_index
    )
    # 4. í˜„ì¬ ì„ íƒëœ ê°’ì„ localStorageì— ì €ì¥
    # ì‚¬ìš©ìê°€ ê°’ì„ ë³€ê²½í•˜ë©´ Streamlitì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ, 
    # ì´ ì½”ë“œëŠ” í•­ìƒ ìµœì‹  ì„ íƒ ê°’ì„ ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.
    localS.setItem("selected_model", [selected_category,selected_item])
   

    st.divider()
    st.header("MCP ì„œë²„ ê´€ë¦¬")
    mcp_config = load_mcp_config()
    with st.expander("ì„œë²„ ëª©ë¡ ë³´ê¸°/ê´€ë¦¬"):
        st.json(mcp_config, expanded=False)
        servers = list(mcp_config["mcpServers"].keys())
        server_to_delete = st.selectbox("ì‚­ì œí•  ì„œë²„ ì„ íƒ", [""] + servers)
        if st.button("ì„ íƒëœ ì„œë²„ ì‚­ì œ", type="primary"):
            if server_to_delete and server_to_delete in mcp_config["mcpServers"]:
                del mcp_config["mcpServers"][server_to_delete]
                save_mcp_config(mcp_config)
                st.success(f"'{server_to_delete}' ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                time.sleep(1); st.rerun()
        st.markdown("---")
        st.write("**ì„œë²„ ìŠ¤ìœ„ì¹˜**")
        server_configs = mcp_config.get("mcpServers", {})
        config_changed = False
        for server_name, config in server_configs.items():
            is_active = st.toggle(
                server_name,
                value=config.get("active", True),
                key=f"toggle_{server_name}"
            )
            if is_active != config.get("active", True):
                mcp_config["mcpServers"][server_name]["active"] = is_active
                config_changed = True
        if config_changed:
            save_mcp_config(mcp_config)
            st.toast("ì„œë²„ í™œì„±í™” ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.markdown("---")
        st.write("**ìƒˆ ì„œë²„ ì¶”ê°€**")
        new_server_name = st.text_input("ìƒˆ ì„œë²„ ì´ë¦„")
        new_server_config_str = st.text_area("ìƒˆ ì„œë²„ JSON ì„¤ì •", height=200, placeholder='{\n  "description": "...",\n ...}')
        if st.button("ìƒˆ ì„œë²„ ì¶”ê°€"):
            if new_server_name and new_server_config_str:
                try:
                    new_config = json.loads(new_server_config_str)
                    mcp_config["mcpServers"][new_server_name] = new_config
                    save_mcp_config(mcp_config)
                    st.success(f"'{new_server_name}' ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1); st.rerun()
                except json.JSONDecodeError: st.error("ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            else: st.warning("ì„œë²„ ì´ë¦„ê³¼ ì„¤ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()
    st.header("ì €ì¥ëœ ëŒ€í™”")
    saved_chats = sorted([f for f in os.listdir(HISTORY_DIR) if f.endswith(".json")], reverse=True)
    if not saved_chats:
        st.write("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    for filename in saved_chats:
        col1, col2 = st.columns([0.85, 0.15])
        with col1:
            is_active_chat = st.session_state.get("current_chat_file") == filename
            button_type = "primary" if is_active_chat else "secondary"
            if st.button(filename, key=f"load_{filename}", use_container_width=True, type=button_type):
                if not is_active_chat:
                    load_chat(filename)
                    st.rerun()
        with col2:
            if st.button("X", key=f"delete_{filename}", use_container_width=True, help=f"{filename} ì‚­ì œ"):
                delete_chat(filename)
                st.rerun()

# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_chat_file" not in st.session_state:
    st.session_state.current_chat_file = None

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

st.markdown(
    """
    <style>
    @media(max-width:1024px){
        .stBottom{
        bottom:60px;
        }
    }
    </style>
    """,unsafe_allow_html=True
)
# if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if prompt:
    if not st.session_state.get("current_chat_file"):
        st.session_state.current_chat_file = generate_filename_with_timestamp()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        history = [
            HumanMessage(content=m['content']) if m['role'] == 'user' else AIMessage(content=m['content'])
            for m in st.session_state.messages[:-1]
        ]
        response = st.write_stream(process_query(prompt, history))
        st.badge("Answer by "+selected_item+"", icon=":material/check:", color="green")

    st.session_state.messages.append({"role": "assistant", "content": response})
    auto_save_chat()