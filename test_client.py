"""
chatbot_long_memory.py (RAMâ€‘only ë²„ì „)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¶ RUN :  python chatbot_long_memory.py

âœ”ï¸ í•µì‹¬ ëª©í‘œ  
  â€¢ ê¸°ì¡´ ReAct ì˜ˆì œ(toolâ€‘callback ë¡œê¹…) ê·¸ëŒ€ë¡œ ìœ ì§€  
  â€¢ **í”„ë¡œì„¸ìŠ¤ë¥¼ ì¬ì‹œì‘í•˜ë©´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”** (ëŒ€í™” ê¸°ë¡ì´ ë‚¨ì§€ ì•ŠìŒ)  

ğŸ› ï¸ ì„¤ì¹˜:
```bash
pip install -U langgraph langchain-openai langchain-core \
               langchain-mcp-adapters tiktoken python-dotenv
```
`.env` ë˜ëŠ” OS í™˜ê²½ ë³€ìˆ˜ì— **OPENAI_API_KEY**ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
import uuid   
import asyncio
from pathlib import Path
from dotenv import load_dotenv
import json
from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain.callbacks.base import AsyncCallbackHandler
from langgraph.checkpoint.memory import MemorySaver  # ğŸ—‘ï¸ RAMâ€‘only ì²´í¬í¬ì¸í„°
from langchain_core.messages import HumanMessage

######################################################################
# 0. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
######################################################################

load_dotenv()
BASE_DIR = Path(__file__).parent  # í˜„ì¬ ë””ë ‰í„°ë¦¬ ì‚¬ìš© â€“ íŒŒì¼ ì €ì¥ ì—†ìŒ
SESSION_ID = str(uuid.uuid4())      # ì¬ì‹œì‘í•˜ë©´ ìƒˆ ID â†’ RAM ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì»¨ì…‰ê³¼ ì˜ ë§ìŒ
######################################################################
# 1. Callback í•¸ë“¤ëŸ¬ (ë³€ê²½ ì—†ìŒ)
######################################################################

class ToolPrintHandler(AsyncCallbackHandler):
    async def on_tool_start(self, serialized, input_str, *, run_id, **kwargs):
        name = serialized.get("name", "unknown_tool") if isinstance(serialized, dict) else str(serialized)
        print(f"ğŸ”§  {name} called with â†’ {input_str}")

    async def on_tool_end(self, output, *, run_id, **kwargs):
        print(f"âœ…  tool returned â† {output}")

######################################################################
# 2. MCP ì„œë²„ ëª©ë¡ (ì˜ˆì‹œ ê·¸ëŒ€ë¡œ)
######################################################################
with open('mcp.json','r') as f:
    SERVERS = json.load(f)

######################################################################
# 3. ì—ì´ì „íŠ¸ + ë¹„íœ˜ë°œì„± ë©”ëª¨ë¦¬ ì œê±° (MemorySaverë§Œ ì‚¬ìš©)
######################################################################

checkpointer = MemorySaver()  # ğŸ“Œ ì¢…ë£Œ ì‹œ ëª¨ë“  ìƒíƒœê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤

async def build_agent():
    model = ChatOpenAI(model="gpt-4o-mini", streaming=True)
    client = MultiServerMCPClient(SERVERS)
    tools = await client.get_tools()
    agent = create_react_agent(model, tools, checkpointer=checkpointer)
    print("íˆ´ ëª©ë¡:", [t.name for t in tools])
    return agent

######################################################################
# 4. ë©”ì¸ ë£¨í”„
######################################################################

async def main():
    agent = await build_agent()
    handler = ToolPrintHandler()

    print("ğŸ”— ì—°ê²°ë¨! 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ (ì¬ì‹œì‘í•˜ë©´ ê¸°ë¡ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤)")

    while True:
        question = input("ğŸ‘¤  ")
        if question.lower() == "exit":
            break

        result = await agent.ainvoke(
            {
                # âœ… create_react_agent ê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹
                "messages": [HumanMessage(content=question)],
            },
            config={
                "callbacks": [handler],
                "configurable": {"thread_id": SESSION_ID},  # âœ… MemorySaver ìš”êµ¬ì‚¬í•­
            },
        )

        print("\nğŸ’¬ Final answer:", result["messages"][-1].content)

if __name__ == "__main__":
    asyncio.run(main())
